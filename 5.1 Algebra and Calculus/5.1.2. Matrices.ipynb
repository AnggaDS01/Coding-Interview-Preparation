{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [E]asy: **Matriks sebagai Transformasi Linear**\n",
    "1. **Kenapa matriks disebut transformasi linear?**\n",
    "   Matriks bisa dilihat sebagai operator yang mengubah vektor input jadi vektor output dengan cara linier. Misal, kalau ada matriks **A** dan vektor **x**, transformasi yang dilakukan matriks **A** ke vektor **x** adalah $ \\mathbf{A} \\mathbf{x} $. Operasi ini disebut linier karena memenuhi dua sifat utama dari transformasi linier:\n",
    "   \n",
    "   - **Additivity**: $ \\mathbf{A}(\\mathbf{x} + \\mathbf{y}) = \\mathbf{A}\\mathbf{x} + \\mathbf{A}\\mathbf{y} $\n",
    "   - **Homogeneity**: $ \\mathbf{A}(c\\mathbf{x}) = c\\mathbf{A}\\mathbf{x} $ untuk skalar $ c $.\n",
    "   \n",
    "   Dalam ML, transformasi linier ini dasar banget. Contohnya waktu kita punya layer dense di neural network, itu sebenernya operasi matriks $ \\mathbf{W} \\mathbf{x} $.\n",
    "\n",
    "### [E]asy: **Inverse Matriks**\n",
    "2. **Apa itu invers dari matriks?**\n",
    "   Invers dari matriks **A** adalah matriks lain, misal **A⁻¹**, yang kalau dikalikan dengan **A** akan menghasilkan matriks identitas $ \\mathbf{I} $, yaitu:\n",
    "\n",
    "   $\n",
    "   \\mathbf{A} \\mathbf{A}^{-1} = \\mathbf{I}\n",
    "   $\n",
    "   \n",
    "   Tapi nggak semua matriks punya invers. Matriks punya invers kalau dia **non-singular** atau **determinant-nya nggak nol**. Kalau determinant-nya nol, artinya matriks itu singular, jadi nggak ada inversnya.\n",
    "\n",
    "   **Aplikasi di ML**: Invers sering dipakai di algoritma seperti **Normal Equation** dalam **Linear Regression** buat nyari solusi optimal:\n",
    "\n",
    "   $\n",
    "   \\mathbf{\\hat{w}} = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\mathbf{y}\n",
    "   $\n",
    "\n",
    "3. **Apakah invers selalu unik?**\n",
    "   Ya, kalau sebuah matriks punya invers, maka invers itu selalu unik.\n",
    "\n",
    "### [E]asy: **Determinant Matriks**\n",
    "4. **Apa arti determinant dari matriks?**\n",
    "   Determinant ngasih kita informasi tentang **volume scaling factor** dari transformasi yang dilakukan oleh matriks. Kalau determinant-nya nol, berarti matriks \"meratakan\" ruang, alias ada dimensi yang ilang (singular).\n",
    "\n",
    "5. **Apa yang terjadi ke determinant kalau salah satu baris matriks dikali skalar?**\n",
    "   Kalau lo kali satu baris dengan skalar $ k $, determinant matriks juga akan dikali dengan $ k $. Jadi, kalau determinant awalnya $ \\text{det}(A) $, setelah satu baris dikali $ k $, determinant baru jadi $ k \\times \\text{det}(A) $.\n",
    "\n",
    "### [M]edium: **Eigenvalues**\n",
    "6. **Matriks dengan empat eigenvalue, apa yang bisa kita bilang tentang trace dan determinant?**\n",
    "   Eigenvalues ngasih lo informasi langsung tentang **trace** dan **determinant** dari matriks:\n",
    "\n",
    "   - **Trace** dari matriks adalah jumlah dari semua eigenvalue. Jadi, kalau lo punya empat eigenvalue $ \\lambda_1, \\lambda_2, \\lambda_3, \\lambda_4 $, trace matriksnya adalah $ \\text{Tr}(A) = \\lambda_1 + \\lambda_2 + \\lambda_3 + \\lambda_4 $.\n",
    "   - **Determinant** adalah hasil kali dari semua eigenvalue. Jadi $ \\text{det}(A) = \\lambda_1 \\times \\lambda_2 \\times \\lambda_3 \\times \\lambda_4 $.\n",
    "\n",
    "### [M]edium: **Determinant Matriks**\n",
    "7. **Apa yang bisa kita katakan tentang determinant matriks tanpa ngitung langsung?**\n",
    "   Misal kita diberi matriks yang punya **struktur khusus**, seperti **diagonal matrix** atau **orthogonal matrix**. Tanpa ngitung langsung, lo bisa tau:\n",
    "   \n",
    "   - Kalau matriks **diagonal**, determinant adalah hasil kali semua elemen diagonalnya.\n",
    "   - Kalau matriks **orthogonal**, determinantnya bisa langsung $ \\pm 1 $ karena matriks orthogonal melestarikan panjang dan sudut vektor.\n",
    "\n",
    "### [M]edium: **Covariance Matrix vs Gram Matrix**\n",
    "8. **Apa beda covariance matrix dan Gram matrix?**\n",
    "   - **Covariance matrix**: Ini mengukur bagaimana dua variabel berubah bersama. Buat dataset **X** berukuran $n \\times d$, covariance matrix-nya adalah $ \\frac{1}{n} \\mathbf{X}^\\top \\mathbf{X} $. Covariance matrix dipakai buat ngitung korelasi antar fitur.\n",
    "   - **Gram matrix**: Ini adalah produk titik semua vektor dalam dataset. Kalau **X** adalah dataset, Gram matrix adalah $ \\mathbf{X}^\\top \\mathbf{X} $. Ini dipakai di **kernel methods** kayak SVM buat ngitung similaritas antara data.\n",
    "\n",
    "### [E]asy: **Solusi Unik**\n",
    "9. **Kapan sistem persamaan matriks punya solusi unik?**\n",
    "   Sistem $ \\mathbf{Ax} = \\mathbf{b} $ punya solusi unik kalau matriks **A** adalah non-singular (inversible), artinya determinant-nya nggak nol.\n",
    "\n",
    "### [M]edium: **Multiple Solutions**\n",
    "10. **Kenapa sistem dengan lebih banyak kolom daripada baris punya banyak solusi?**\n",
    "    Kalau matriks **A** punya lebih banyak kolom (fitur) daripada baris (sampel), itu artinya lo punya **underdetermined system**, yang artinya ada banyak cara buat nyari solusi. Secara geometris, ini kayak punya ruang solusi yang lebih besar daripada constraint yang lo punya, jadi ada banyak solusi.\n",
    "\n",
    "### [M]edium: **Matriks Tak Invers**\n",
    "11. **Gimana lo menyelesaikan sistem $ \\mathbf{Ax} = \\mathbf{b} $ kalau matriks **A** nggak punya invers?**\n",
    "    Kalau matriks **A** nggak punya invers (singular), lo bisa pakai **pseudoinverse** buat dapet solusi terbaik secara **least squares**. Pseudoinverse $ \\mathbf{A}^+ $ dihitung dengan:\n",
    "\n",
    "    $\n",
    "    \\mathbf{A}^+ = (\\mathbf{A}^\\top \\mathbf{A})^{-1} \\mathbf{A}^\\top\n",
    "    $\n",
    "    \n",
    "    Ini sering dipakai buat nyelesain sistem yang nggak bisa diselesaikan secara langsung, terutama kalau ada noise di datanya.\n",
    "\n",
    "---\n",
    "\n",
    "### [E]asy: **Derivative**\n",
    "1. **Apa itu turunan (derivative)?**\n",
    "   Turunan adalah cara ngukur seberapa cepat fungsi berubah. Kalau lo punya fungsi $ f(x) $, turunannya $ f'(x) $ adalah tingkat perubahan $ f(x) $ terhadap $ x $. Secara sederhana, turunan ngasih tahu lo \"seberapa miring\" grafik di suatu titik.\n",
    "\n",
    "### [M]edium: **Derivative, Gradient, Jacobian**\n",
    "2. **Apa beda derivative, gradient, dan Jacobian?**\n",
    "   - **Derivative**: Kalau lo punya fungsi satu variabel, turunannya $ f'(x) $ adalah laju perubahan fungsi itu terhadap variabelnya.\n",
    "   - **Gradient**: Kalau lo punya fungsi dengan banyak variabel $ f(\\mathbf{x}) $, gradient adalah vektor dari semua turunan parsial terhadap setiap variabel. Misal, untuk fungsi $ f(x_1, x_2) $, gradient-nya:\n",
    "\n",
    "     $\n",
    "     \\nabla f = \\left( \\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2} \\right)\n",
    "     $\n",
    "   - **Jacobian**: Kalau lo punya vektor fungsi $ \\mathbf{f}(\\mathbf{x}) $, Jacobian adalah matriks yang mengandung semua turunan parsial fungsi-fungsi itu terhadap variabel-variabelnya."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
