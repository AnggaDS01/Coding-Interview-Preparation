{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oke bro, kita lanjut bahas soal tentang **Dimensionality Reduction**. Ini penting banget di Machine Learning, terutama buat dataset yang ukurannya gede banget dengan banyak fitur. Gue bakal mulai dari yang paling dasar dulu.\n",
    "\n",
    "### [E]asy: **Kenapa kita butuh Dimensionality Reduction?**\n",
    "1. **Kenapa kita perlu pengurangan dimensi?**\n",
    "   Pengurangan dimensi dilakukan supaya kita bisa mengurangi jumlah fitur tanpa mengorbankan terlalu banyak informasi penting. Alasan utamanya:\n",
    "   - **Mengurangi overfitting**: Dengan terlalu banyak fitur, model bisa jadi terlalu kompleks dan overfit, artinya performa bagus di training tapi jelek di data baru.\n",
    "   - **Meningkatkan efisiensi komputasi**: Kalau datanya banyak, model butuh waktu dan resource lebih besar buat dilatih.\n",
    "   - **Visualisasi**: Manusia lebih gampang memahami data dalam 2D atau 3D. Dengan reduksi dimensi, kita bisa plot data yang aslinya berdimensi tinggi jadi sesuatu yang lebih gampang dilihat.\n",
    "   \n",
    "   Contohnya, **PCA (Principal Component Analysis)** sering dipakai buat reduksi dimensi.\n",
    "\n",
    "### [E]asy: **Eigendecomposition**\n",
    "2. **Apakah eigendecomposition dari matriks selalu unik?**\n",
    "   Nggak selalu unik. Eigendecomposition adalah teknik buat memecah matriks jadi eigenvalue dan eigenvector-nya. Namun, kalau ada eigenvalue yang berulang (degenerate eigenvalues), eigenvector-nya bisa jadi nggak unik. Eigenvalue tetap unik, tapi vektor yang sesuai bisa punya lebih dari satu representasi.\n",
    "\n",
    "### [M]edium: **Aplikasi Eigenvalues dan Eigenvectors**\n",
    "3. **Sebutkan beberapa aplikasi dari eigenvalue dan eigenvector?**\n",
    "   Eigenvalue dan eigenvector punya banyak aplikasi di machine learning:\n",
    "   - **PCA (Principal Component Analysis)**: Kita pake eigenvalue buat ngurutin komponen mana yang paling signifikan.\n",
    "   - **Graph Theory**: Eigenvalue digunakan buat menganalisa struktur graf, misal di PageRank Google.\n",
    "   - **Spectral Clustering**: Eigenvalue dipakai buat ngereduksi dimensi graf sebelum clustering.\n",
    "\n",
    "### [M]edium: **PCA dengan Fitur yang Berbeda Skala**\n",
    "4. **Apakah PCA akan bekerja pada dataset dengan fitur dalam skala berbeda?**\n",
    "   Kalau fitur-fitur dalam dataset punya skala yang beda jauh (misal satu dalam range 0-1, yang lain 10-1000), PCA bisa kasih hasil yang nggak optimal. Kenapa? Karena PCA tergantung sama varians, dan fitur dengan range besar akan mendominasi variansnya. Solusinya adalah melakukan **normalisasi atau standardisasi** dulu sebelum PCA, misal dengan scaling fitur-fitur supaya punya distribusi dengan mean 0 dan variance 1.\n",
    "\n",
    "### [H]ard: **Kapan bisa pakai Eigendecomposition dan SVD?**\n",
    "5. **Di kondisi apa kita bisa pakai eigendecomposition? Apa hubungannya dengan SVD?**\n",
    "   - **Eigendecomposition** hanya bisa dipakai untuk **square matrices** (matriks persegi).\n",
    "   - **SVD (Singular Value Decomposition)** adalah generalisasi dari eigendecomposition, dan bisa digunakan pada **rectangular matrices** (matriks non-persegi). SVD membagi matriks jadi tiga bagian: $ \\mathbf{A} = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^\\top $, di mana:\n",
    "     - $ \\mathbf{U} $ dan $ \\mathbf{V} $ adalah matriks orthogonal.\n",
    "     - $ \\mathbf{\\Sigma} $ adalah matriks diagonal yang berisi singular values.\n",
    "\n",
    "   Hubungan antara **PCA** dan **SVD** adalah bahwa PCA sebenernya bisa dihitung dengan SVD. Fitur utama dari PCA adalah menemukan komponen utama yang maksimal memuat varians data, dan ini bisa dilakukan dengan menghitung singular values dari matriks data.\n",
    "\n",
    "### [H]ard: **t-SNE**\n",
    "6. **Gimana cara kerja t-SNE dan kenapa kita butuh?**\n",
    "   **t-SNE (T-distributed Stochastic Neighbor Embedding)** adalah metode reduksi dimensi yang cocok buat **visualisasi** data berdimensi tinggi ke dalam 2D atau 3D. t-SNE bekerja dengan memetakan poin-poin yang mirip di dimensi tinggi ke jarak yang lebih dekat di dimensi rendah, dan poin-poin yang berbeda lebih jauh dipisahkan.\n",
    "\n",
    "   - t-SNE bekerja dengan membangun distribusi probabilitas yang memodelkan hubungan tetangga terdekat dalam dimensi tinggi dan rendah. Dengan cara ini, t-SNE bisa lebih sensitif terhadap kluster di data dibandingkan PCA.\n",
    "   - Kenapa kita butuh? t-SNE sangat bagus buat menangkap struktur lokal di data, yang nggak selalu ketangkep sama PCA atau SVD, terutama buat **manifold learning** di data non-linear.\n",
    "\n",
    "---\n",
    "\n",
    "**Analogi tentang PCA dan SVD:**\n",
    "Misal lo punya rak buku besar dengan ratusan buku, tapi lo cuma mau simpen buku-buku yang paling sering lo baca. PCA ngebantu lo buat milih buku mana yang punya variasi paling banyak dari semua koleksi lo. Sedangkan SVD bisa dibilang cara yang lebih fleksibel buat ngesort buku-buku itu, nggak harus disusun rapi, tapi tetep bisa ngegambar representasi terbaik.\n",
    "\n",
    "Ada lagi yang mau lo tanyain soal dimensionality reduction?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
